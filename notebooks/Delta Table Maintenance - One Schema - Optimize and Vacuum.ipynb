{"cells":[{"cell_type":"markdown","source":["# Delta Table Maintenance\n","This notebook is used to query spark for the listing of Delta Tables so they can be automatically optimized and vacuumed.  For more on this topic, check out https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-table-maintenance"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6425c77-2633-4611-ba7c-252c58929f44"},{"cell_type":"markdown","source":["## Define Schema information\n","Use this cell to **define only one schema** in your Lakehouse for the OPTIMIZE and VACUUM commands."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5498b57f-a45a-48b5-a37f-6bda5c78d2d9"},{"cell_type":"code","source":["from pyspark.context import SparkContext\n","from pyspark.sql.session import SparkSession\n","from notebookutils import mssparkutils\n","\n","sparkContext = SparkContext.getOrCreate()\n","spark = SparkSession(sparkContext)\n","\n","retention_hours = '168' # This equals 1 week\n","schema_name = 'dbo'  # Replace with your actual schema name\n","\n","# Initialize counters\n","total_tables = 0\n","optimized_tables = 0\n","vacuumed_tables = 0\n","\n","# First, count total tables across all schemas\n","# for schema_name in schemas:\n","try:\n","    tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n","    table_count = tables_df.count()\n","    print(f\"Found {table_count} tables in schema {schema_name}\")\n","    display(tables_df)\n","    total_tables += table_count\n","    table_column_name = \"tableName\"\n","    print(f\"Using column '{table_column_name}' for table names\")\n","\n","except Exception as e:\n","    print(f\"Error listing tables in schema {schema_name}: {str(e)}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ac406a49-de69-49aa-a660-3c58b250c8a6"},{"cell_type":"markdown","source":["## Optimize tables in the schema declared above\n","VOrder is applied.  For more on this topic, check out https://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e1e629f-d4f2-41d8-9d9c-2557d40821f2"},{"cell_type":"code","source":["from delta.tables import DeltaTable\n","\n","try:\n","    tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n","    tables = tables_df.collect()\n","    print(f\"Optimizing tables in {schema_name}...\")\n","\n","    for table in tables:\n","        try:\n","            table_name = table[table_column_name]\n","            full_table_name = f\"{schema_name}.`{table_name}`\"\n","\n","            try:\n","                spark.sql(f\"OPTIMIZE {full_table_name} VORDER\")\n","                optimized_tables += 1\n","                print(f\"✔ Optimized using SQL: {full_table_name} ({optimized_tables}/{total_tables})\")\n","            except Exception as e1:\n","                try:\n","                    deltaTable = DeltaTable.forName(spark, full_table_name)\n","                    deltaTable.optimize().executeCompaction()\n","                    optimized_tables += 1\n","                    print(f\"✔ Optimized using DeltaTable: {full_table_name} ({optimized_tables}/{total_tables})\")\n","                except Exception as e2:\n","                    print(f\"✖ Failed to optimize '{full_table_name}': {str(e2)}\")\n","        except Exception as e:\n","            print(f\"⚠ Error processing table in schema {schema_name}: {str(e)}\")\n","except Exception as e:\n","    print(f\"⚠ Error processing schema {schema_name}: {str(e)}\")\n","\n","print(f\"✅ Optimization complete. Successfully optimized {optimized_tables} out of {total_tables} tables.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"068065e3-e4f9-41b7-9987-704db44774f2"},{"cell_type":"markdown","source":["## Vacuum tables in the schema declared above"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"94c15575-ec33-4b6a-b064-98dd0227e70e"},{"cell_type":"code","source":["from delta.tables import DeltaTable\n","\n","try:\n","    tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n","    tables = tables_df.collect()\n","    for table in tables:\n","        try:\n","            table_name = table[table_column_name]\n","            full_table_name = f\"{schema_name}.`{table_name}`\"\n","\n","            try:\n","                spark.sql(f\"VACUUM {full_table_name} RETAIN {retention_hours} HOURS\")\n","                vacuumed_tables += 1\n","                print(f\"✔ Vacuumed using SQL: {full_table_name} ({vacuumed_tables}/{total_tables})\")\n","            except Exception as e1:\n","                try:\n","                    deltaTable = DeltaTable.forName(spark, full_table_name)\n","                    deltaTable.vacuum(retention_hours)\n","                    vacuumed_tables += 1\n","                    print(f\"✔ Vacuumed using DeltaTable: {full_table_name} ({vacuumed_tables}/{total_tables})\")\n","                except Exception as e2:\n","                    print(f\"✖ Failed to vacuum '{full_table_name}': {str(e2)}\")\n","        except Exception as e:\n","            print(f\"⚠ Error processing table in schema {schema_name}: {str(e)}\")\n","except Exception as e:\n","    print(f\"⚠ Error processing schema {schema_name}: {str(e)}\")\n","\n","print(f\"✅ Vacuuming complete. Successfully vacuumed {vacuumed_tables} out of {total_tables} tables.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8e2080a5-8c10-44ed-a99c-a60f9779c2e2"},{"cell_type":"markdown","source":["Verify OPTIMIZE and VACUUM command executed on a specific table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4dc85a70-c90e-4e53-89c6-07166201b153"},{"cell_type":"code","source":["%%sql\n","DESCRIBE HISTORY dbo.products"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"aa3d40da-f884-405f-a28f-a53f7f69f357"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"d432f040-850c-47a7-a9f3-75c68110b8ac"}],"default_lakehouse":"d432f040-850c-47a7-a9f3-75c68110b8ac","default_lakehouse_name":"Northwind","default_lakehouse_workspace_id":"3dd7bd5f-a174-4dec-acd3-570371570ea2"}}},"nbformat":4,"nbformat_minor":5}