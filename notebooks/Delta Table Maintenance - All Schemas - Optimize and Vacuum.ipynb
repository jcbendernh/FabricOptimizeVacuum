{"cells":[{"cell_type":"markdown","source":["# Delta Table Maintenance\n","This notebook is used to query spark for the listing of Delta Tables so they can be automatically optimized and vacuumed.  For more on this topic, check out https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-table-maintenance"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6425c77-2633-4611-ba7c-252c58929f44"},{"cell_type":"markdown","source":["## Gather Schema information"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1c69cfb3-c902-477e-b5ba-e5a3643a02a2"},{"cell_type":"code","source":["from delta.tables import *\n","from tqdm.auto import tqdm\n"," \n","# Get schemas\n","schemas_df = spark.sql(\"SHOW SCHEMAS\")\n","print(\"Schema DataFrame columns:\", schemas_df.columns)\n"," \n","# Extract just the schema names from the fully qualified paths\n","raw_schemas = [row[\"namespace\"] for row in schemas_df.collect()]\n","print(f\"Raw schemas: {raw_schemas}\")\n"," \n","# Extract just the schema name from the fully qualified path\n","schemas = []\n","for raw_schema in raw_schemas:\n","    # Remove backticks and split by dots\n","    parts = raw_schema.replace('`', '').split('.')\n","    # The last part should be the actual schema name\n","    if len(parts) >= 3:\n","        schema_name = parts[-1]\n","        schemas.append(schema_name)\n"," \n","print(f\"Extracted schema names: {schemas}\")\n","total_tables = 0\n","optimized_tables = 0\n","vacuumed_tables = 0\n"," \n","# First, count total tables across all schemas\n","for schema_name in schemas:\n","    try:\n","        tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n","        table_count = tables_df.count()\n","        print(f\"Found {table_count} tables in schema {schema_name}\")\n","        display(tables_df)\n","        total_tables += table_count\n","        table_column_name = \"tableName\"\n","        print(f\"Using column '{table_column_name}' for table names\")\n","    except Exception as e:\n","        print(f\"Error listing tables in schema {schema_name}: {str(e)}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ac406a49-de69-49aa-a660-3c58b250c8a6"},{"cell_type":"markdown","source":["## Optimize tables across all schemas\n","VOrder is applied.  For more on this topic, check out https://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"34179b36-2f9e-4440-89ab-8c5be5cc788e"},{"cell_type":"code","source":["from delta.tables import DeltaTable\n","\n","for schema_name in schemas:\n","    try:\n","        tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n","        tables = tables_df.collect()\n","        print(f\"Optimizing tables in schema: {schema_name}\")\n","\n","        for table in tables:\n","            try:\n","                table_name = table[table_column_name]\n","                full_table_name = f\"{schema_name}.`{table_name}`\"\n","\n","                try:\n","                    spark.sql(f\"OPTIMIZE {full_table_name} VORDER\")\n","                    optimized_tables += 1\n","                    print(f\"✔ Optimized using SQL: {full_table_name} ({optimized_tables}/{total_tables})\")\n","                except Exception as e1:\n","                    try:\n","                        deltaTable = DeltaTable.forName(spark, full_table_name)\n","                        deltaTable.optimize().executeCompaction()\n","                        optimized_tables += 1\n","                        print(f\"✔ Optimized using DeltaTable: {full_table_name} ({optimized_tables}/{total_tables})\")\n","                    except Exception as e2:\n","                        print(f\"✖ Failed to optimize '{full_table_name}': {str(e2)}\")\n","            except Exception as e:\n","                print(f\"⚠ Error processing table in schema {schema_name}: {str(e)}\")\n","    except Exception as e:\n","        print(f\"⚠ Error processing schema {schema_name}: {str(e)}\")\n","\n","print(f\"✅ Optimization complete. Successfully optimized {optimized_tables} out of {total_tables} tables.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d7dfd6ee-97c7-4445-a32e-0c2c77b3b583"},{"cell_type":"markdown","source":["## Vacuum tables across all schemas"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b9520f7c-c8af-400c-b149-23b3ee320704"},{"cell_type":"code","source":["from delta.tables import DeltaTable\n","\n","retention_hours = '168'  # This equals 1 week\n","\n","for schema_name in schemas:\n","    try:\n","        tables_df = spark.sql(f\"SHOW TABLES IN {schema_name}\")\n","        tables = tables_df.collect()\n","        print(f\"Vacuuming tables in schema: {schema_name}\")\n","        \n","        for table in tables:\n","            try:\n","                table_name = table[table_column_name]\n","                full_table_name = f\"{schema_name}.`{table_name}`\"\n","                try:\n","                    spark.sql(f\"VACUUM {full_table_name} RETAIN {retention_hours} HOURS\")\n","                    vacuumed_tables += 1\n","                    print(f\"✔ Vacuumed using SQL: {full_table_name} ({vacuumed_tables}/{total_tables})\")\n","                except Exception as e1:\n","                    try:\n","                        deltaTable = DeltaTable.forName(spark, full_table_name)\n","                        deltaTable.vacuum(retention_hours)\n","                        vacuumed_tables += 1\n","                        print(f\"✔ Vacuumed using DeltaTable: {full_table_name} ({vacuumed_tables}/{total_tables})\")\n","                    except Exception as e2:\n","                        print(f\"✖ Failed to vacuum '{full_table_name}': {str(e2)}\")\n","            except Exception as e:\n","                print(f\"⚠ Vacuum - Error processing table in schema {schema_name}: {str(e)}\")\n","    except Exception as e:\n","        print(f\"⚠ Error processing schema {schema_name}: {str(e)}\")\n","\n","print(f\"✅ Optimization complete. Successfully vacuumed {vacuumed_tables} out of {total_tables} tables.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4efec216-29c4-4511-9b1c-66d42c8f0e3f"},{"cell_type":"markdown","source":["## Verify OPTIMIZE and VACUUM command executed on a specific table\n","You will need to change the schema and table to a valid schema.table in your Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6a9b77ab-a960-4134-81ae-986bbd72ef16"},{"cell_type":"code","source":["%%sql\n","DESCRIBE HISTORY nwd.Customers"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"aa3d40da-f884-405f-a28f-a53f7f69f357"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"d432f040-850c-47a7-a9f3-75c68110b8ac"}],"default_lakehouse":"d432f040-850c-47a7-a9f3-75c68110b8ac","default_lakehouse_name":"Northwind","default_lakehouse_workspace_id":"3dd7bd5f-a174-4dec-acd3-570371570ea2"}}},"nbformat":4,"nbformat_minor":5}