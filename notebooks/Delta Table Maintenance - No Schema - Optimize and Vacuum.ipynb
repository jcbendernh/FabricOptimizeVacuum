{"cells":[{"cell_type":"markdown","source":["# Delta Table Maintenance\n","This notebook is used to query spark for the listing of Delta Tables so they can be automatically optimized and vacuumed.  For more on this topic, check out https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-table-maintenance"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6425c77-2633-4611-ba7c-252c58929f44"},{"cell_type":"markdown","source":["## Define Variables\n","Use this cell to **define your variables** in your Lakehouse for the OPTIMIZE and VACUUM commands."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5498b57f-a45a-48b5-a37f-6bda5c78d2d9"},{"cell_type":"code","source":["from pyspark.context import SparkContext\n","from pyspark.sql.session import SparkSession\n","from notebookutils import mssparkutils\n","\n","sparkContext = SparkContext.getOrCreate()\n","spark = SparkSession(sparkContext)\n","\n","retention_hours = '168' # This equals 1 week\n","\n","# Initialize counters\n","total_tables = 0\n","optimized_tables = 0\n","vacuumed_tables = 0\n","\n","# First, count total tables across all schemas\n","# for schema_name in schemas:\n","try:\n","    tables_df = spark.sql(f\"SHOW TABLES\")\n","    table_count = tables_df.count()\n","    print(f\"Found {table_count} tables\")\n","    display(tables_df)\n","    total_tables += table_count\n","    table_column_name = \"tableName\"\n","    print(f\"Using column '{table_column_name}' for table names\")\n","except Exception as e:\n","    print(f\"Error listing tables: {str(e)}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ac406a49-de69-49aa-a660-3c58b250c8a6"},{"cell_type":"markdown","source":["## Optimize tables\n","VOrder is applied.  For more on this topic, check out https://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e1e629f-d4f2-41d8-9d9c-2557d40821f2"},{"cell_type":"code","source":["from delta.tables import DeltaTable\n","\n","try:\n","    tables_df = spark.sql(\"SHOW TABLES\")\n","    tables = tables_df.collect()\n","    print(\"Starting optimization of tables...\")\n","\n","    for table in tables:\n","        try:\n","            table_name = table[table_column_name]\n","\n","            try:\n","                spark.sql(f\"OPTIMIZE {table_name} VORDER\")\n","                optimized_tables += 1\n","                print(f\"✔ Optimized using SQL: {table_name} ({optimized_tables}/{total_tables})\")\n","            except Exception as e1:\n","                try:\n","                    deltaTable = DeltaTable.forName(spark, table_name)\n","                    deltaTable.optimize().executeCompaction()\n","                    optimized_tables += 1\n","                    print(f\"✔ Optimized using DeltaTable: {table_name} ({optimized_tables}/{total_tables})\")\n","                except Exception as e2:\n","                    print(f\"✖ Failed to optimize '{table_name}': {str(e2)}\")\n","        except Exception as e:\n","            print(f\"⚠ Optimize - Error processing table: {str(e)}\")\n","except Exception as e:\n","    print(f\"⚠ Error processing: {str(e)}\")\n","\n","print(f\"✅ Optimization complete. Successfully optimized {optimized_tables} out of {total_tables} tables.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"068065e3-e4f9-41b7-9987-704db44774f2"},{"cell_type":"markdown","source":["## Vacuum tables"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"94c15575-ec33-4b6a-b064-98dd0227e70e"},{"cell_type":"code","source":["from delta.tables import DeltaTable\n","\n","try:\n","    tables_df = spark.sql(\"SHOW TABLES\")\n","    tables = tables_df.collect()\n","    print(\"Starting vacuum process...\")\n","\n","    for table in tables:\n","        try:\n","            table_name = table[table_column_name]\n","\n","            try:\n","                spark.sql(f\"VACUUM {table_name} RETAIN {retention_hours} HOURS\")\n","                vacuumed_tables += 1\n","                print(f\"✔ Vacuumed using SQL: {table_name} ({vacuumed_tables}/{total_tables})\")\n","            except Exception as e1:\n","                try:\n","                    deltaTable = DeltaTable.forName(spark, table_name)\n","                    deltaTable.vacuum(retention_hours)\n","                    vacuumed_tables += 1\n","                    print(f\"✔ Vacuumed using DeltaTable: {table_name} ({vacuumed_tables}/{total_tables})\")\n","                except Exception as e2:\n","                    print(f\"✖ Failed to vacuum '{table_name}': {str(e2)}\")\n","        except Exception as e:\n","            print(f\"⚠ Vacuum - Error processing table: {str(e)}\")\n","except Exception as e:\n","    print(f\"⚠ Error processing: {str(e)}\")\n","\n","print(f\"✅ Optimization complete. Successfully vacuumed {vacuumed_tables} out of {total_tables} tables.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7452660c-4146-4b81-9a3a-a9df4196a1b1"},{"cell_type":"markdown","source":["Verify OPTIMIZE and VACUUM command executed on a specific table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4dc85a70-c90e-4e53-89c6-07166201b153"},{"cell_type":"code","source":["%%sql\n","DESCRIBE HISTORY customers"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"aa3d40da-f884-405f-a28f-a53f7f69f357"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"version":"0.1","state":{}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"f58a322a-164b-4f10-8199-158a03c91230"}],"default_lakehouse":"f58a322a-164b-4f10-8199-158a03c91230","default_lakehouse_name":"Silver","default_lakehouse_workspace_id":"fb1bc16b-71f0-4c77-a92a-b0ec8058a6de"}}},"nbformat":4,"nbformat_minor":5}